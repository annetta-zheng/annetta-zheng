---
layout: post
title: Hackathon - MongoDB NYC AI Hackathon - LivEYE 
tags: data-science machine-learning natural-language-processing
categories: [Data Science, Hackthon/Challenge]
---
Empowering the Visual Impaired People Through Soundscapes with a video-to-voice model.

In this application, we built a video-to-voice model to generate live descriptions for low vision or blind people when they need to go out. The model decomposes the video into images and generates detailed descriptions of the route, aiding low vision or blind people to live better lives.

[![](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/LydiaZhou/Mongo-DB-Hackathon "Click for Repo!") 

Frame Capping:
Our app utilizes advanced video-to-audio technology, converting real-time video into sequential frames.

Stable Diffusion:
Textual information is extracted from these frames, providing contextual details about the environment.

ElevenLabs:
This extracted text is then translated into immersive audio cues, allowing blind users to perceive and interact with their surroundings effectively.


<iframe src='https://docs.google.com/presentation/d/13gEAvZqDkxrEA1azFC0_RGGbXIRjisezwh2rCkad5LI/edit?usp=sharing' width='100%' height='600px' frameborder='0'>

